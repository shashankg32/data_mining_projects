{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "965a8e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as mp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec16f0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28479, 31)\n"
     ]
    }
   ],
   "source": [
    "#Import Dataset\n",
    "dataset=pd.read_csv(\"/home/shashank/data_mining/dm-la-1-fraud-detecion-dt-creditcard-data.csv\")\n",
    "dataset=dataset.sample(frac=0.1,random_state=1)\n",
    "print(dataset.shape)\n",
    "x=dataset.iloc[:,1:30].values\n",
    "y=dataset.iloc[:,30].values"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d56fcf3",
   "metadata": {},
   "source": [
    "This code reads a CSV file called \"creditcard.csv\" using the pandas library and stores it in the variable \"dataset\". The code then takes a random sample of 10% of the dataset using the \"sample\" function and sets the random state to 1. It then prints the shape of the resulting dataset.\n",
    "\n",
    "The next two lines of code separate the features (columns 1 to 30) into a variable called \"x\" and the target variable (column 30) into a variable called \"y\". This is done using the \"iloc\" function to select the appropriate columns and the \"values\" function to convert the resulting data frames to numpy arrays.# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c34bb97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28479, 29)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57638432",
   "metadata": {},
   "source": [
    "The output (28479, 29) refers to the shape of the Pandas DataFrame dataset, which has 28481 rows and 31 columns.\n",
    "\n",
    "In more detail, 28481 represents the number of observations or rows in the dataset, and 31 represents the number of features or columns in the dataset. The first column of the dataset (column 0) is the index column, and the remaining 30 columns (columns 1-30) represent the features or attributes of each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8dc9b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28479,)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66167566",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing values\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer=SimpleImputer(missing_values=np.nan,strategy='mean')\n",
    "imputer.fit(x[:,1:30])\n",
    "x[:,1:30]=imputer.transform(x[:,1:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3499aa20",
   "metadata": {},
   "source": [
    "This code is using the SimpleImputer class from the scikit-learn library to handle missing values in the dataset.\n",
    "\n",
    "First, the SimpleImputer is initialized with two arguments:\n",
    "- missing_values: the type of value in the dataset that represents a missing value. In this case, it is numpy.nan, which is a special value that represents not-a-number or missing value in NumPy arrays.\n",
    "- strategy: the strategy to use to replace the missing values. In this case, the 'mean' strategy is used, which replaces the missing values with the mean of the non-missing values in the same column.\n",
    "\n",
    "Next, the fit() method of the imputer object is called to compute the mean of each column in the dataset, which will be used to replace the missing values.\n",
    "\n",
    "Then, the transform() method of the imputer object is called to replace the missing values in the dataset with the mean of each column. The transform() method returns a new array with the missing values replaced.\n",
    "\n",
    "Finally, the transformed data is assigned back to the same variable x to update the original dataset with the imputed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdecec89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28479, 29)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d76aec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting dataset into tranning and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc76207",
   "metadata": {},
   "source": [
    "This code is splitting the preprocessed dataset `x` and corresponding labels `y` into training and testing sets for model building and evaluation purposes. \n",
    "\n",
    "The `train_test_split` function is imported from the `sklearn.model_selection` module, which allows for easy splitting of datasets. \n",
    "\n",
    "The function takes four parameters:\n",
    "\n",
    "- `x`: the input dataset\n",
    "- `y`: the corresponding labels for the input dataset\n",
    "- `test_size`: the proportion of the dataset to include in the testing set\n",
    "- `random_state`: a seed value for the random number generator used by the function to ensure reproducibility of the split\n",
    "\n",
    "The function returns four arrays:\n",
    "- `xtrain`: the training set input data\n",
    "- `xtest`: the testing set input data\n",
    "- `ytrain`: the training set labels\n",
    "- `ytest`: the testing set labels\n",
    "\n",
    "These sets can be used to train a model on the training set, and then evaluate its performance on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53580bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain.shape :  (21359, 29)\n",
      "xtest.shape  :  (7120, 29)\n",
      "ytrain.shape :  (21359,)\n",
      "xtest.shape  :  (7120, 29)\n"
     ]
    }
   ],
   "source": [
    "print(\"xtrain.shape : \", xtrain.shape)\n",
    "print(\"xtest.shape  : \", xtest.shape)\n",
    "print(\"ytrain.shape : \", ytrain.shape)\n",
    "print(\"xtest.shape  : \", xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fe5b12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fatures Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "xtrain = sc.fit_transform(xtrain)\n",
    "xtest = sc.transform(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84faee3c",
   "metadata": {},
   "source": [
    "This code performs feature scaling on the training and testing data using the StandardScaler from the sklearn.preprocessing module. Feature scaling is a technique used to standardize the range of input features so that each feature contributes equally to the model's performance. \n",
    "\n",
    "First, the StandardScaler object is created. Then, the fit_transform method of the StandardScaler object is called on the training data to fit the scaler to the data and transform it. The resulting scaled data is stored in the xtrain variable. The transform method is then called on the testing data using the fitted scaler object to scale the testing data using the same scaling factor as the training data. The resulting scaled testing data is stored in the xtest variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff836da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardised Training Set : \n",
      " [ 0.1755644  -0.71248176 -0.31583312  0.85960078  0.01465336  0.49792678\n",
      "  0.55263113 -0.06872708 -0.0259695  -0.24648807 -1.23350634  0.25705763\n",
      "  0.98690252  0.22150697  1.47365492  0.53144905 -0.88433372 -0.4011437\n",
      " -0.38480945  1.19318005 -0.07850928 -1.61777284 -0.74765221 -2.18692915\n",
      "  0.46867959 -1.268875   -0.11169069  0.23662413  1.91620047]\n"
     ]
    }
   ],
   "source": [
    "print(\"Standardised Training Set : \\n\", xtrain[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14caf175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', random_state=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Decision Tree Classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408af19e",
   "metadata": {},
   "source": [
    "This code is performing Decision Tree Classification using the `DecisionTreeClassifier` class from the `sklearn.tree` module. \n",
    "\n",
    "Here's a breakdown of the parameters used:\n",
    "- `criterion = 'entropy'`: This specifies the criterion to be used for splitting. In this case, it's entropy, which is a measure of the impurity of a node.\n",
    "- `random_state = 0`: This sets the random seed for reproducibility.\n",
    "\n",
    "The `fit` method is then used to train the decision tree classifier on the training set (`xtrain` and `ytrain`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95897f32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7103    4]\n",
      " [   7    6]]\n",
      "Accuracy_Decison    :  99.84550561797752\n",
      "Error_rate  :  0.1544943820224719\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred=classifier.predict(xtest)\n",
    "cm = confusion_matrix(ytest, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(ytest, y_pred)\n",
    "Accuracy_Decison = ((cm[0][0] + cm[1][1]) / cm.sum()) *100\n",
    "print(\"Accuracy_Decison    : \", Accuracy_Decison)\n",
    "Error_rate = ((cm[0][1] + cm[1][0]) / cm.sum()) *100\n",
    "print(\"Error_rate  : \", Error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f12dff",
   "metadata": {},
   "source": [
    "Decision Tree Accuracy :\n",
    "Accuracy_Decison : 99.87361325656508 -- (A better approach to follow)\n",
    "Error_rate : 0.12638674343491083"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c7fca5",
   "metadata": {},
   "source": [
    "# new input with output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdb4b68",
   "metadata": {},
   "source": [
    "10, 1.44904378114715, -1.17633882535966, 0.913859832832795, -1.37566665499943, -1.97138316545323, -0.62915213889734, -1.4232356010359, 0.048455887908856, -1.72040839292037, 1.62665905834133, 1.1996439495421, -0.671439778462005, -0.513947152539479, -0.095045045399955, 0.230930409124119, 0.031967466786208, 0.253414715863197, 0.854343814324194, -0.221365413645481, -0.387226474431156, -0.009301896524901, 0.313894410791098, 0.027740158017025, 0.500512287104917, 0.25136735874921, -0.129477953726618, 0.042849870938146, 0.016253261937552, 7.8 = This input is not fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f544752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new data point\n",
    "new_data_point = np.array([1.44904378114715, -1.17633882535966, 0.913859832832795, -1.37566665499943, -1.97138316545323, -0.62915213889734, -1.4232356010359, 0.048455887908856, -1.72040839292037, 1.62665905834133, 1.1996439495421, -0.671439778462005, -0.513947152539479, -0.095045045399955, 0.230930409124119, 0.031967466786208, 0.253414715863197, 0.854343814324194, -0.221365413645481, -0.387226474431156, -0.009301896524901, 0.313894410791098, 0.027740158017025, 0.500512287104917, 0.25136735874921, -0.129477953726618, 0.042849870938146, 0.016253261937552, 7.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "789bdaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the new input data to numpy array\n",
    "new_data_point = np.array(new_data_point)\n",
    "# standardize the new data point using the same StandardScaler used for training the model\n",
    "new_data_point_std = sc.fit_transform(new_data_point.reshape(1, -1))\n",
    "# \n",
    "new_data_point_std_a = sc.transform(new_data_point_std)\n",
    "#\n",
    "prediction = classifier.predict(new_data_point_std_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bba888db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This transaction is not fraudulent...\n"
     ]
    }
   ],
   "source": [
    "# print the prediction\n",
    "if prediction == 0:\n",
    "    print(\"This transaction is not fraudulent...\")\n",
    "else:\n",
    "    print(\"This transaction is fraud...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda (base)",
   "language": "python",
   "name": "anaconda-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
